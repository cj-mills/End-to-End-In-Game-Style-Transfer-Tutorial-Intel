{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Style_Transfer_Tutorial_Intel.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"widgets":{"application/vnd.jupyter.widget-state+json":{"453a756223f44e5e82a1a67e26dbf1bb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4d105f2f2c9f4875a9cebc0fd2137c1d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_66e6c656efc2433a81e22bc9c938fd18","IPY_MODEL_7a8974b153ff425284f12d9eb63439c6"]}},"4d105f2f2c9f4875a9cebc0fd2137c1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"66e6c656efc2433a81e22bc9c938fd18":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_65a32a5d9acb4cec9de00ed6dbbd7708","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":574673361,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":574673361,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2da10d14c64f4c63bbc1290fbe99ec69"}},"7a8974b153ff425284f12d9eb63439c6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_001de2e7f57344f984464127bd88b65c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 548M/548M [01:09&lt;00:00, 8.23MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_150589dfa7b04768bc642553eabca32a"}},"65a32a5d9acb4cec9de00ed6dbbd7708":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2da10d14c64f4c63bbc1290fbe99ec69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"001de2e7f57344f984464127bd88b65c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"150589dfa7b04768bc642553eabca32a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"fO7HWxqTAzNp"},"source":["**Important:** Google Colab restricts GPU allocation for free users to 12 hours at a time. You will get disconnected from the server if you leave this notebook running past that. You need to wait a while (probably 12 hours) for the time limit to reset."]},{"cell_type":"markdown","metadata":{"id":"69bYigdcg90A"},"source":["## Install the [fastai](https://docs.fast.ai/) Library\n","\n","First, we'll install the fastai library which is built on top of PyTorch. We'll be using pure PyTorch for training the model but the fastai library includes some convenience functions that we'll use to download the training dataset."]},{"cell_type":"code","metadata":{"id":"-Q83PMa6b-RU"},"source":["%%capture\n","!pip install fastai==2.2.5"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KhnbdLSdnIBM"},"source":["## Import Dependencies\n","Next, we need to import the required python modules and packages."]},{"cell_type":"code","metadata":{"id":"yFbPJHlZcQHw"},"source":["# Miscellaneous operating system interfaces\n","# https://docs.python.org/3/library/os.html\n","import os\n","# Time access and conversions\n","# https://docs.python.org/3/library/time.html\n","import time\n","# Object-oriented filesystem paths\n","# https://docs.python.org/3/library/pathlib.html#pathlib.Path\n","from pathlib import Path\n","# Tuple-like objects that have named fields\n","# https://docs.python.org/3/library/collections.html#collections.namedtuple\n","from collections import namedtuple\n","\n","# A convenience function for downloading files from a url to a destination folder\n","# https://docs.fast.ai/data.external.html#untar_data\n","from fastai.data.external import untar_data\n","\n","# Provides image processing capabilities\n","# https://pillow.readthedocs.io/en/stable/reference/Image.html\n","from PIL import Image\n","\n","# The main PyTorch package\n","# https://pytorch.org/docs/stable/torch.html\n","import torch\n","\n","# Used to iterate over the dataset during training \n","# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n","from torch.utils.data import DataLoader\n","\n","# Contains definitions of models. We'll be downloading a pretrained VGG-19 model\n","# to judge the performance of our style transfer model.\n","# https://pytorch.org/vision/stable/models.html#torchvision.models.vgg19\n","from torchvision.models import vgg19\n","# Common image transforms that we'll use to process images before feeding them to the models\n","# https://pytorch.org/vision/stable/transforms.html\n","from torchvision import transforms\n","# Loads images from a directory and applies the specified transforms\n","# https://pytorch.org/vision/stable/datasets.html#imagefolder\n","from torchvision.datasets import ImageFolder"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EP8IuH98oT14"},"source":["## Utility Functions\n","\n","We'll define some utility functions for making new directories, loading and saving images, and stylizing images using model checkpoints."]},{"cell_type":"code","metadata":{"id":"_6fV_AxBoMQl"},"source":["def make_dir(dir_name: str):\n","    \"\"\"Create the specified directory if it doesn't already exist\"\"\"\n","    dir_path = Path(dir_name)\n","    try:\n","        dir_path.mkdir()\n","    except:\n","        print(\"Directory already exists.\")\n","\n","def load_image(filename: str, size: int=None, scale: float=None):\n","    \"\"\"Load the specified image and return it as a PIL Image\"\"\"\n","    img = Image.open(filename)\n","    if size is not None:\n","        img = img.resize((size, size), Image.ANTIALIAS)\n","    elif scale is not None:\n","        img = img.resize((int(img.size[0] / scale), int(img.size[1] / scale)), Image.ANTIALIAS)\n","    return img\n","\n","def save_image(filename: str, data: torch.Tensor):\n","    \"\"\"Save the Tensor data to an image file\"\"\"\n","    img = data.clone().clamp(0, 255).numpy()\n","    img = img.transpose(1, 2, 0).astype(\"uint8\")\n","    img = Image.fromarray(img)\n","    img.save(filename)\n","\n","def load_checkpoint(model_path):\n","    state_dict = torch.load(model_path)\n","    keys = [k for k in state_dict.keys()]\n","    filters = set()\n","    filters_list = [state_dict[k].shape[0] for k in keys if not (state_dict[k].shape[0] in filters or filters.add(state_dict[k].shape[0]))]\n","    res_blocks = len(set(k.split('.')[1] for k in state_dict.keys() if 'resnets' in k))\n","    model = TransformerNet(filters=filters_list[:-1], res_blocks=res_blocks) \n","    model.load_state_dict(state_dict, strict=False)\n","    return model\n","\n","def stylize(model_path: str, input_image: str, output_image: str, content_scale: float=None, \n","            device: str=\"cpu\", export_onnx: bool=None):\n","    \"\"\"Load a TransformerNet checkpoint, stylize an image and save the output\"\"\"\n","    device = torch.device(device)\n","    content_image = load_image(input_image, scale=content_scale)\n","    content_transform = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Lambda(lambda x: x.mul(255))\n","    ])\n","    content_image = content_transform(content_image)\n","    content_image = content_image.unsqueeze(0).to(device)\n","\n","    with torch.no_grad():\n","        style_model = load_checkpoint(model_path)\n","        style_model.to(device)\n","         \n","        if export_onnx:\n","            assert export_onnx.endswith(\".onnx\"), \"Export model file should end with .onnx\"\n","            output = torch.onnx._export(style_model, content_image, export_onnx, opset_version=9).cpu()\n","        else:\n","            output = style_model(content_image).cpu()\n","    save_image(output_image, output[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7zTTy7Bmd4xf"},"source":["## Define the Style Transfer Model\n","\n","Here we'll define the style transfer model itself. The model takes in an RGB image and generates a new image with the same dimensions. The features in the output image (e.g. color and texure) are then compared with the features of the style image and content image. The results of these comparisons are then used to update the parameters of the model so that it hopefully generates better images.\n","\n","I won't go into detail about the model architecture as the goal of this tutorial is primarily showing how to use it."]},{"cell_type":"code","metadata":{"id":"-qkcKSj3oxPQ"},"source":["class TransformerNet(torch.nn.Module):\n","    \"\"\"TransformerNet\n","    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L4\n","    \"\"\"\n","    \n","    def __init__(self, filters=(32, 64, 128), res_blocks=5):\n","        super(TransformerNet, self).__init__()\n","        self.filters = filters\n","        self.res_blocks = res_blocks if res_blocks > 0 else 1\n","        # Initial convolution layers\n","        self.conv1 = ConvLayer(3, filters[0], kernel_size=9, stride=1)\n","        self.in1 = torch.nn.InstanceNorm2d(filters[0], affine=True)\n","        self.conv2 = ConvLayer(filters[0], filters[1], kernel_size=3, stride=2)\n","        self.in2 = torch.nn.InstanceNorm2d(filters[1], affine=True)\n","        self.conv3 = ConvLayer(filters[1], filters[2], kernel_size=3, stride=2)\n","        self.in3 = torch.nn.InstanceNorm2d(filters[2], affine=True)\n","        # Residual layers\n","        self.resnets = torch.nn.ModuleList()\n","        for i in range(self.res_blocks):\n","            self.resnets.append(ResidualBlock(filters[2]))\n","        \n","        # Upsampling Layers\n","        self.deconv1 = UpsampleConvLayer(filters[2], filters[1], kernel_size=3, stride=1, upsample=2)\n","        self.in4 = torch.nn.InstanceNorm2d(filters[1], affine=True)\n","        self.deconv2 = UpsampleConvLayer(filters[1], filters[0], kernel_size=3, stride=1, upsample=2)\n","        self.in5 = torch.nn.InstanceNorm2d(filters[0], affine=True)\n","        self.deconv3 = ConvLayer(filters[0], 3, kernel_size=9, stride=1)\n","        # Non-linearities\n","        self.relu = torch.nn.ReLU()\n","        \n","    def forward(self, X):\n","        conv1_y = self.relu(self.in1(self.conv1(X)))\n","        conv2_y = self.relu(self.in2(self.conv2(conv1_y)))\n","        conv3_y = self.relu(self.in3(self.conv3(conv2_y)))\n","\n","        y = self.resnets[0](conv3_y) + conv3_y\n","        \n","        for i in range(1, self.res_blocks):\n","            y = self.resnets[i](y) + y\n","\n","        y = self.relu(self.in4(self.deconv1(conv3_y + y)))\n","        y = self.relu(self.in5(self.deconv2(conv2_y + y)))\n","        y = self.deconv3(conv1_y + y)\n","        return y\n","\n","\n","class ConvLayer(torch.nn.Module):\n","    \"\"\"ConvLayer\n","    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L44\n","    \"\"\"\n","\n","    def __init__(self, in_channels, out_channels, kernel_size, stride):\n","        super(ConvLayer, self).__init__()\n","        reflection_padding = kernel_size // 2\n","        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n","        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n","\n","    def forward(self, x):\n","        out = self.reflection_pad(x)\n","        out = self.conv2d(out)\n","        return out\n","\n","\n","class ResidualBlock(torch.nn.Module):\n","    \"\"\"ResidualBlock\n","    introduced in: https://arxiv.org/abs/1512.03385\n","    recommended architecture: http://torch.ch/blog/2016/02/04/resnets.html\n","    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L57\n","    \"\"\"\n","\n","    def __init__(self, channels):\n","        super(ResidualBlock, self).__init__()\n","        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n","        self.in1 = torch.nn.InstanceNorm2d(channels, affine=True)\n","        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n","        self.in2 = torch.nn.InstanceNorm2d(channels, affine=True)\n","        self.relu = torch.nn.ReLU()\n","      \n","    def forward(self, x):\n","        residual = x\n","        out = self.relu(self.in1(self.conv1(x)))\n","        out = self.in2(self.conv2(out))\n","        out = out + residual\n","        return out\n","\n","\n","class UpsampleConvLayer(torch.nn.Module):\n","    \"\"\"UpsampleConvLayer\n","    Upsamples the input and then does a convolution. This method gives better results\n","    compared to ConvTranspose2d.\n","    ref: http://distill.pub/2016/deconv-checkerboard/\n","    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/transformer_net.py#L79\n","    \"\"\"\n","\n","    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n","        super(UpsampleConvLayer, self).__init__()\n","        self.upsample = upsample\n","        reflection_padding = kernel_size // 2\n","        self.reflection_pad = torch.nn.ReflectionPad2d(reflection_padding)\n","        self.conv2d = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n","        \n","    def forward(self, x):\n","        x_in = x\n","        if self.upsample:\n","            x_in = torch.nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n","        out = self.reflection_pad(x_in)\n","        out = self.conv2d(out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LDlQpf-SAzN0"},"source":["## Define the VGG-19 Model\n","Next, we'll define the model that will be used to judge the quality of the output images from the style transfer model. This model has been pretrained a large image dataset. This means it's already learned to recognize a wide variety of features in images. We'll use this model to extract the features of the content image, style image, and stylized images."]},{"cell_type":"code","metadata":{"id":"hEBFOVxrs6md"},"source":["class Vgg19(torch.nn.Module):\n","    \"\"\"\n","    https://github.com/pytorch/examples/blob/36441a83b6595524a538e342594ee6482754f374/fast_neural_style/neural_style/vgg.py#L7\n","    \"\"\"\n","    \n","    def __init__(self, requires_grad=False):\n","        super(Vgg19, self).__init__()\n","        self.feature_layers = [0, 3, 5]\n","        self.vgg_pretrained_features = vgg19(pretrained=True).features\n","        self.slice1 = torch.nn.Sequential()\n","        self.slice2 = torch.nn.Sequential()\n","        self.slice3 = torch.nn.Sequential()\n","        self.slice4 = torch.nn.Sequential()\n","        self.slice5 = torch.nn.Sequential()\n","        for x in range(4):\n","            self.slice1.add_module(str(x), self.vgg_pretrained_features[x])\n","        for x in range(4, 9):\n","            self.slice2.add_module(str(x), self.vgg_pretrained_features[x])\n","        for x in range(9, 18):\n","            self.slice3.add_module(str(x), self.vgg_pretrained_features[x])\n","        for x in range(18, 27):\n","            self.slice4.add_module(str(x), self.vgg_pretrained_features[x])\n","        for x in range(27, 36):\n","            self.slice5.add_module(str(x), self.vgg_pretrained_features[x])\n","        if not requires_grad:\n","            for param in self.parameters():\n","                param.requires_grad = False\n","            \n","    def forward(self, X):\n","        h = self.slice1(X)\n","        h_relu1_2 = h\n","        h = self.slice2(h)\n","        h_relu2_2 = h\n","        h = self.slice3(h)\n","        h_relu3_3 = h\n","        h = self.slice4(h)\n","        h_relu4_3 = h\n","        h = self.slice5(h)\n","        h_relu5_3 = h\n","        vgg_outputs = namedtuple(\"VggOutputs\", ['relu1_2', 'relu2_2', 'relu3_3', 'relu4_3', 'relu5_3'])\n","        out = vgg_outputs(h_relu1_2, h_relu2_2, h_relu3_3, h_relu4_3, h_relu5_3)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DMSP4mAYoVHP"},"source":["## Define the Model Trainer\n","\n","We'll define a new class to make training the style transfer model a bit easier. Along with training the model, this class will save the model's current progress at set intervals. It will also generate sample images so we can see how the model is doing. This will allow us to determine if the model is actually improving or whether it's already good enough that we can stop the training process early."]},{"cell_type":"code","metadata":{"id":"kbwOOyEloAiu"},"source":["class Trainer(object):\n","    def __init__(self, train_loader, style_transform, generator, opt_generator, style_criterion, perception_model, device):\n","        self.train_loader = train_loader\n","        self.style_transform = style_transform\n","        self.generator = generator\n","        self.opt_generator = opt_generator\n","        self.style_criterion = style_criterion\n","        self.perception_model = perception_model\n","        self.device = device\n","        self.generator.to(self.device)\n","        \n","    def gram_matrix(self, y: torch.Tensor):\n","        \"\"\"Compute the gram matrix a PyTorch Tensor\"\"\"\n","        (b, ch, h, w) = y.size()\n","        features = y.view(b, ch, w * h)\n","        features_t = features.transpose(1, 2)\n","        gram = features.bmm(features_t) / (ch * h * w)\n","        return gram\n","\n","    def normalize_batch(self, batch: torch.Tensor):\n","        \"\"\"Normalize a batch of Tensors using the imagenet mean and std \"\"\"\n","        mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n","        std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n","        batch = batch.div_(255.0)\n","        return (batch - mean) / std\n","\n","    def get_gram_style(self, style_image: str, style_size: int):\n","        \"\"\"Get the Gram Matrices for the style image\"\"\"\n","        style = load_image(style_image, size=style_size)\n","        style = self.style_transform(style)\n","        style = style.repeat(self.train_loader.batch_size, 1, 1, 1).to(self.device)\n","        features_style = self.perception_model(self.normalize_batch(style))\n","        gram_style = [self.gram_matrix(y) for y in features_style]\n","        return gram_style\n","            \n","    def save_checkpoint(self, path: str):\n","        \"\"\"Save the current model weights at the specified path\"\"\"\n","        self.generator.eval().cpu()\n","        torch.save(self.generator.state_dict(), path)\n","        print(f\"Checkpoint saved at {path}\")\n","\n","    def train(self, style_image, test_image, checkpoint_model_dir, epochs=5, content_weight=1e5, style_weight=1e10, \n","                content_scale=None, style_size=None, log_interval=500, checkpoint_interval=500):\n","        \"\"\"Train the style transfer model on the provided style image.\"\"\"\n","        \n","        gram_style = self.get_gram_style(style_image, style_size)\n","\n","        for e in range(epochs):\n","            self.generator.train()\n","            agg_content_loss = 0.\n","            agg_style_loss = 0.\n","            count = 0\n","            for batch_id, (x, _) in enumerate(self.train_loader):\n","                n_batch = len(x)\n","                count += n_batch\n","                self.opt_generator.zero_grad()\n","                \n","                x = x.to(self.device)\n","                y = self.generator(x)\n","\n","                y = self.normalize_batch(y.clone())\n","                x = self.normalize_batch(x.clone())\n","                features_y = self.perception_model(y)\n","                features_x = self.perception_model(x)\n","\n","                content_loss = content_weight * self.style_criterion(features_y.relu2_2, features_x.relu2_2)\n","\n","                style_loss = 0.\n","                for ft_y, gm_s in zip(features_y, gram_style):\n","                    gm_y = self.gram_matrix(ft_y)\n","                    style_loss += self.style_criterion(gm_y, gm_s[:n_batch, :, :])\n","                style_loss = style_loss * style_weight\n","\n","                total_loss = content_loss + style_loss\n","                total_loss.backward()\n","                self.opt_generator.step()\n","\n","                agg_content_loss += content_loss.item()\n","                agg_style_loss += style_loss.item()\n","\n","                if (batch_id + 1) % log_interval == 0:\n","                    mesg = f\"{' '.join(time.ctime().replace('  ', ' ').split(' ')[1:-1])}  \"\n","                    mesg += f\"Epoch {e + 1}: [{count}/{len(self.train_loader.dataset)}]  \"\n","                    mesg += f\"content: {(agg_content_loss / (batch_id + 1)):.4f}  \"\n","                    mesg += f\"style: {(agg_style_loss / (batch_id + 1)):.4f}  \"\n","                    mesg += f\"total: {((agg_content_loss + agg_style_loss) / (batch_id + 1)):.4f}\"\n","                    print(mesg)\n","\n","                if checkpoint_model_dir is not None and (batch_id + 1) % checkpoint_interval == 0:\n","                    ckpt_base = f\"ckpt_epoch_{e}_batch_id_{batch_id + 1}\"\n","                    ckpt_model_filename = ckpt_base + \".pth\"\n","                    ckpt_model_path = os.path.join(checkpoint_model_dir, ckpt_model_filename)\n","                    self.save_checkpoint(ckpt_model_path)\n","                    output_image = ckpt_base + \".png\"\n","                    output_image_path = os.path.join(checkpoint_model_dir, output_image)\n","                    stylize(ckpt_model_path, test_image, output_image_path)\n","                    self.generator.to(self.device).train()\n","                \n","        print(\"Finished Training\")\n","        ckpt_model_path = os.path.join(checkpoint_model_dir, 'final.pth')\n","        self.save_checkpoint(ckpt_model_path)\n","        output_image_path = os.path.join(checkpoint_model_dir, 'final.png')\n","        stylize(ckpt_model_path, test_image, output_image_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ij0DfpBp7vuR"},"source":["## Mount Google Drive\n","\n","Before going any further, we need to mount out Google Drive so we can access our project folder. There is a python library that's specifically made for working in Colab notebook that provides this functionality."]},{"cell_type":"code","metadata":{"id":"Zdag21BajLZp"},"source":["from google.colab import drive"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E1LvKEf--Pxz"},"source":["The default directory for colab environments is `/content/` as shown below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MNH24XuQ-Ckp","executionInfo":{"status":"ok","timestamp":1614719186603,"user_tz":480,"elapsed":587,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"fe070001-d8ea-41ea-d5e3-2a4ae3bd1e58"},"source":["!pwd"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QYBKVecU-M4O"},"source":["We'll use the `drive.mount()` method to mount our whole Google Drive inside a new directory  called `drive`.\n","\n","When you run the code cell below, you will be prompted to open a link to allow Google Colab to access your Drive.\n","\n","Once you allow access you will be provided with an authorization code. Copy and paste the code into text box that appears in the output of the code cell and press Enter."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9rnggAX4-JYA","executionInfo":{"status":"ok","timestamp":1614740141177,"user_tz":480,"elapsed":20565,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"3bbf0bc5-61f0-4923-ad99-d4897bcfb36d"},"source":["drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"btYjkf2V-idG"},"source":["If we look in the new `drive` folder, we can see that our main Drive folder is named `MyDrive`. All the folders and files in your Drive are accessible in `MyDrive`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cskniodqcQBP","executionInfo":{"status":"ok","timestamp":1614719215062,"user_tz":480,"elapsed":460,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"f139a8fb-5efa-4820-8615-794cd869c7ea"},"source":["!ls ./drive/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["MyDrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"msOaiiBt-gE4"},"source":["If you placed and named your project folder as shown in part 1 of this tutorial, it should be located at `/content/drive/MyDrive/Style_Transfer_Project`.\n","\n","We'll need that path to our project folder to store in Python variables in the next section."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O9o4lFC0ehBl","executionInfo":{"status":"ok","timestamp":1614719216499,"user_tz":480,"elapsed":299,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"12850f08-a5d6-4081-c652-271997e545e3"},"source":["!ls /content/drive/MyDrive/Style_Transfer_Project"],"execution_count":null,"outputs":[{"output_type":"stream","text":["checkpoints  movie_001.mp4  style_images  test_images\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qt7R6NgvnhmO"},"source":["## Set the Directories\n","\n","Now we need to create several variables to store the paths to various directories.\n","* The dataset directory\n","* The Google Drive style transfer project directory\n","* The style images directory\n","* The test image directory\n","* The model checkpoint directory\n","\n","The dataset directory will be on the Google Colab environment while the rest will be on your Google Drive. This will allow you to keep all your progress while preventing the dataset from filling up your Drive storage.\n","\n","I recommend creating separate checkpoint directories for each training session. That makes it easier to compare results from experiments."]},{"cell_type":"code","metadata":{"id":"oSXvYLvFcQN6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614740142294,"user_tz":480,"elapsed":534,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"46aa83ed-65f9-4b5c-b000-f67e471c6304"},"source":["dataset_dir = \"/content/dataset\"\n","\n","project_dir = '/content/drive/MyDrive/Style_Transfer_Project'\n","style_images_dir = f\"{project_dir}/style_images\"\n","test_images_dir = f\"{project_dir}/test_images\"\n","checkpoints_dir = f\"{project_dir}/checkpoints\"\n","make_dir(checkpoints_dir)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Directory already exists.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qhCScLGUcQR1","executionInfo":{"status":"ok","timestamp":1614719221503,"user_tz":480,"elapsed":316,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"0c0ba959-c10f-4fa7-b874-3d5ffc5536f5"},"source":["!ls $project_dir"],"execution_count":null,"outputs":[{"output_type":"stream","text":["checkpoints  movie_001.mp4  style_images  test_images\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t1AI1ROHnlRZ"},"source":["## Download Training Dataset\n","\n","We'll be using the [COCO](https://cocodataset.org/#home) train 2014 image dataset to train our model. It's about 13.5 GB unzipped. That's just high enough to trigger the disk space warning without actually using up the available disk space. You will likely get a disk space warning while the dataset is being unzipped. You can click ignore in the popup window. We'll delete the zip file once the folder is unzipped."]},{"cell_type":"code","metadata":{"id":"pr3vWJoHfzSy","scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1614740841141,"user_tz":480,"elapsed":697118,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"0ac580f7-3e6d-4584-d8b8-fc8d11db18d6"},"source":["coco_url = 'http://images.cocodataset.org/zips/train2014.zip'\n","untar_data(coco_url, 'coco.zip', dataset_dir)\n","if os.path.exists('coco.zip'): os.remove('coco.zip')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":[""],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"jLEfRQCJLzqP"},"source":["The dataset is located in a subdirectory named `train2014` and contains `82,783` images."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-0fnm9NoLbhE","executionInfo":{"status":"ok","timestamp":1614719920980,"user_tz":480,"elapsed":694637,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"97945aad-a74c-4d1b-aee8-422b6934b7d4"},"source":["!ls ./dataset/train2014/ | wc -l"],"execution_count":null,"outputs":[{"output_type":"stream","text":["82783\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lQKzkivBAzN6"},"source":["## Split Gameplay Video\n","\n","In this section we'll split the gameplay video if you made one. We'll store the frames in a new subdirectory called `video_frames` in the `dataset_dir`."]},{"cell_type":"code","metadata":{"id":"S-zJyDl5yn9f"},"source":["!mkdir ./dataset/video_frames/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lZis2H9yAzN7"},"source":["We'll use the `ffmpeg` command-line tool to split the video file. Google Colab should already have the tool installed. \n","\n","In the code cell below replace `/content/drive/MyDrive/Style_Transfer_Project/movie_001.mp4` with the path to your video file.\n","\n","If you recorded a lot of footage, you might want to keep an eye on the available disk space and manually stop the code cell from running. This shouldn't be a problem if you only recorded several minutes of gameplay."]},{"cell_type":"code","metadata":{"id":"jnVP_jfMyq5v","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614741314853,"user_tz":480,"elapsed":1166993,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"3da806cd-7ca5-4dc2-876d-0de0fab2d1ce"},"source":["!ffmpeg -i /content/drive/MyDrive/Style_Transfer_Project/movie_001.mp4 ./dataset/video_frames/%05d.png -hide_banner"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/content/drive/MyDrive/Style_Transfer_Project/movie_001.mp4':\n","  Metadata:\n","    major_brand     : mp42\n","    minor_version   : 0\n","    compatible_brands: mp41isom\n","    creation_time   : 2021-02-25T00:56:33.000000Z\n","  Duration: 00:01:24.03, start: 0.000000, bitrate: 13685 kb/s\n","    Stream #0:0(und): Video: h264 (Constrained Baseline) (avc1 / 0x31637661), yuv420p, 2560x1440 [SAR 1:1 DAR 16:9], 13617 kb/s, 30 fps, 30 tbr, 30k tbn, 60 tbc (default)\n","    Metadata:\n","      creation_time   : 2021-02-25T00:56:33.000000Z\n","      handler_name    : VideoHandler\n","      encoder         : AVC Coding\n","    Stream #0:1(und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, stereo, fltp, 64 kb/s (default)\n","    Metadata:\n","      creation_time   : 2021-02-25T00:56:33.000000Z\n","      handler_name    : SoundHandler\n","Stream mapping:\n","  Stream #0:0 -> #0:0 (h264 (native) -> png (native))\n","Press [q] to stop, [?] for help\n","Output #0, image2, to './dataset/video_frames/%05d.png':\n","  Metadata:\n","    major_brand     : mp42\n","    minor_version   : 0\n","    compatible_brands: mp41isom\n","    encoder         : Lavf57.83.100\n","    Stream #0:0(und): Video: png, rgb24, 2560x1440 [SAR 1:1 DAR 16:9], q=2-31, 200 kb/s, 30 fps, 30 tbn, 30 tbc (default)\n","    Metadata:\n","      creation_time   : 2021-02-25T00:56:33.000000Z\n","      handler_name    : VideoHandler\n","      encoder         : Lavc57.107.100 png\n","frame= 2521 fps=5.3 q=-0.0 Lsize=N/A time=00:01:24.03 bitrate=N/A speed=0.178x    \n","video:5941480kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: unknown\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UNpNAgExyqsf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614720393419,"user_tz":480,"elapsed":1162872,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"0b0971b5-01f4-4959-b2b0-c00abc665d2c"},"source":["!ls /content/dataset/video_frames/ | wc -l\n","!du -h /content/dataset/video_frames/"],"execution_count":null,"outputs":[{"output_type":"stream","text":["2521\n","5.7G\t/content/dataset/video_frames/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"u4kDxfZ2nq3s"},"source":["## Create the Trainer Variables\n","\n","In this section we'll define the variables required to define a new Trainer."]},{"cell_type":"markdown","metadata":{"id":"D_ORbpavspLM"},"source":["### Define the `DataLoader`\n","We need to define a `DataLoader` that will be responsible for iterating through the dataset during training.\n","\n","We also need to specify the `batch_size` which indicates how many images will be fed to the model at a time.\n","\n","Every image in a batch needs to be the same size. We'll set the size using the `image_size` variable.\n","\n","Images need to be processed before being fed to the model. We'll define the preprocessing steps using the `transforms.Compose()` method. Our preprocessing steps include the following:\n","\n","1. Resize the images in the current batch to the target `image_size`\n","2. Crop the images so that they are all square\n","3. Convert the images to PyTorch Tensors\n","4. Multiply the color channel values by 255\n","\n","We then store the list of images in the `dataset_dir` along with the preprocessing steps in a new variable called `train_dataset`.\n","\n","Finally, we create our `DataLoader` using the `train_dataset` and specified `batch_size`"]},{"cell_type":"code","metadata":{"id":"3CU3V_Rpsqr_"},"source":["batch_size = 4\n","image_size = 256\n","transform = transforms.Compose([transforms.Resize(image_size),\n","                                transforms.CenterCrop(image_size),\n","                                transforms.ToTensor(),\n","                                transforms.Lambda(lambda x: x.mul(255))\n","                                ])\n","\n","train_dataset = ImageFolder(dataset_dir, transform)\n","train_loader = DataLoader(train_dataset, batch_size=batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fNIciP5Cn0Th"},"source":["### Select Compute Device\n","\n","We'll double check that a cuda GPU is available using the `torch.cuda.is_available()` method."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lap1HXkUuAwJ","executionInfo":{"status":"ok","timestamp":1614741315235,"user_tz":480,"elapsed":1159488,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"bed95ea4-a149-4f1e-f161-b4e8f50fa634"},"source":["use_cuda = True\n","device = \"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\"\n","print(f\"Using: {device}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using: cuda\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HnXBUPXQojSH"},"source":["### Define Transforms for Style Image\n","\n","Next we'll define the transforms used to process the style image before feeding it to the VGG-19 model. The processing steps are basically the same as for the training images except the style image will have already been resized.\n","\n","1. Convert the image to a PyTorch Tensor\n","2. Multiply the pixel values by 255"]},{"cell_type":"code","metadata":{"id":"rKiv1F9vv24k"},"source":["style_transform = transforms.Compose([transforms.ToTensor(),\n","                                      transforms.Lambda(lambda x: x.mul(255))\n","                                      ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a2U0SBS1pNFp"},"source":["### Create the Style Transfer Model\n","\n","Next, we'll create a new instance of the style transfer model. It's here that you'll be able to experiment with tradeoffs between performance and quality.\n","\n","#### Tuning Model Inference Speed:\n","The easiest way to make the style transfer model faster is to make it smaller. We can easily tune the size of the model by adjusting the size of the layers or by using fewer layers.\n","\n","\n","##### Resolution: `960x540`\n","##### Filters: `(16, 32, 64)`\n","```\n","================================================================\n","Total params: 424,899\n","Trainable params: 424,899\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 5.93\n","Forward/backward pass size (MB): 2210.61\n","Params size (MB): 1.62\n","Estimated Total Size (MB): 2218.17\n","----------------------------------------------------------------\n","```\n","\n","##### Resolution: `960x540`\n","##### Filters: `(32, 64, 128)`\n","```\n","================================================================\n","Total params: 1,679,235\n","Trainable params: 1,679,235\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 5.93\n","Forward/backward pass size (MB): 4385.35\n","Params size (MB): 6.41\n","Estimated Total Size (MB): 4397.69\n","----------------------------------------------------------------\n","```\n","\n","By default, the style transfer model uses the following values:\n","* filters: (32, 64, 128)\n","* res_blocks: 5\n","\n","The `filters` variable determines the size of the layers in the model. The `res_blocks` variable determines the number of `ResidualBlocks` that form the core of the model.\n","\n","I've found that setting filters to `(8, 16, 32)` and keeping res_blocks at `5` significantly improves performance in Unity with minimal impact on quality."]},{"cell_type":"code","metadata":{"id":"XiNEW2rvoxCK"},"source":["filters = (8, 16, 32)\n","res_blocks = 5\n","generator = TransformerNet(filters=filters, res_blocks=res_blocks).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SNz316ugpY5W"},"source":["### Create the Optimizer for the Style Transfer Model \n","Next, we'll define the optimizer for our model. The optimizer determines how the model gets updated during training. The optimizer takes in the model's parameters and a learning rate. The learning rate determines how much the model gets updated after each batch of images.\n","\n","We'll use a learning rate of `1e-3` which is equivalent to `0.001`.\n","\n","**Notation Examples:**\n","* 1e-4 = 0.0001\n","* 1e0 = 1.0\n","* 1e5 = 100000.0\n","* 5e10 = 50000000000.0\n"]},{"cell_type":"code","metadata":{"id":"SResZ2txpZOf"},"source":["lr = 1e-3\n","opt_generator = torch.optim.Adam(generator.parameters(), lr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TA645RtxAzN9"},"source":["### Define How Model Performance Will Be Measured\n","\n","We'll be using Mean Squared Error (MSE) for comparing the difference between the features of the content image and stylized image and between the features of the stylized image and the target style image."]},{"cell_type":"code","metadata":{"id":"y1O6wWIhcqXR"},"source":["style_criterion = torch.nn.MSELoss()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dFEAr3FbAzN9"},"source":["**Note:** If you're not familiar with MSE, take a look at the toy example below.\n","#### Mean Squared Error in Python"]},{"cell_type":"code","metadata":{"id":"0asdNH1rAzN9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614720404361,"user_tz":480,"elapsed":1162139,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"06ea8fca-abcd-4c23-fcfe-14bee4180c52"},"source":["x = [1, 2, 3, 4]\n","y = [5, 6, 7, 8]\n","\n","sum_of_squares = 0\n","for i in range(len(x)):\n","    error = x[i] - y[i]\n","    squared_error = error**2\n","    sum_of_squares += squared_error\n","    \n","mse = sum_of_squares / len(x)\n","mse"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16.0"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"iZRkPag_AzN-"},"source":["#### Mean Squared Error in PyTorch"]},{"cell_type":"code","metadata":{"id":"jc9leD-qAzN-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614720404524,"user_tz":480,"elapsed":1161556,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"304d484e-6237-4e6d-edb0-3e7440c2839f"},"source":["x_t = torch.Tensor(x)\n","y_t = torch.Tensor(y)\n","\n","mse_loss = torch.nn.MSELoss()\n","\n","mse_loss(x_t, y_t)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(16.)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"Jj6Z9ToEAzN-"},"source":["### Create a New VGG-19 Perception Model\n","\n","Next, we'll create a new vgg-19 model. The pretrained model will be downloaded the first time this cell is run."]},{"cell_type":"code","metadata":{"id":"DV5DOst3s6ZD","colab":{"base_uri":"https://localhost:8080/","height":83,"referenced_widgets":["453a756223f44e5e82a1a67e26dbf1bb","4d105f2f2c9f4875a9cebc0fd2137c1d","66e6c656efc2433a81e22bc9c938fd18","7a8974b153ff425284f12d9eb63439c6","65a32a5d9acb4cec9de00ed6dbbd7708","2da10d14c64f4c63bbc1290fbe99ec69","001de2e7f57344f984464127bd88b65c","150589dfa7b04768bc642553eabca32a"]},"executionInfo":{"status":"ok","timestamp":1614741338587,"user_tz":480,"elapsed":1172243,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"a0d9948c-7f0f-4b54-c926-7fa8a24fdbf1"},"source":["perception_model = Vgg19(requires_grad=False).to(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"453a756223f44e5e82a1a67e26dbf1bb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=574673361.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nxVg-Eq0uBCW"},"source":["## Create a New Trainer\n","\n","We can now create a new trainer instance using the variables we defined above."]},{"cell_type":"code","metadata":{"id":"eHajCWW0cqdu"},"source":["trainer = Trainer(train_loader=train_loader, \n","                  style_transform=style_transform, \n","                  generator=generator, \n","                  opt_generator=opt_generator, \n","                  style_criterion=style_criterion, \n","                  perception_model=perception_model, \n","                  device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TUDg6xkDAzN_"},"source":["#### Tuning the Stylized Image\n","\n","The stylized image will be influenced by the following:\n","\n","* Influence of the content image\n","* Influence of the style image\n","* Size of the style image\n","\n","I recommend keeping the content_weight at `1e5` and adjusting the style_weight between `5e8` and `1e11`. The ideal style_weight will vary depending on the style image. I recommend starting out low, training for 5-10 checkpoint intervals, and increasing the style weight as needed."]},{"cell_type":"code","metadata":{"id":"Ne7Xy_r_cpqA"},"source":["# The file path for the target style image\n","style_image = f\"{style_images_dir}/Xe HPG Mesh shader_right_square.jpg\"\n","# The file path for a sample input image for demonstrating the model's progress during training\n","test_image = f\"{test_images_dir}/011.png\" \n","\n","# The number of times to iterate through the entire training dataset\n","epochs = 1\n","\n","# The influence from the input image on the stylized image\n","# Default: 1e5\n","content_weight = 1e5\n","# The influence from the style image on the stylized image\n","# Default: 1e10\n","style_weight = 1e10\n","\n","# (test_image resolution) / content_scale\n","# Default: 1.0\n","content_scale = 0.8\n","# Target size for style_image = (style_size, styl_size)\n","# Default: 256\n","style_size = 720\n","\n","# The number of training batches to wait before printing the progress of the model \n","log_interval = 500\n","# The number of training to wait before saving the current model weights\n","checkpoint_interval = 500"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CD6BP_Nx1em6"},"source":["## Train the Model\n","\n","Once you execute the code cell below, open the checkpoints folder in Google Drive in another tab. You can view the model's progress by looking at the sample style images that get generated with each checkpoint. You can stop the training process early by clicking the stop button where the play button normally is on the left side of the code cell."]},{"cell_type":"code","metadata":{"id":"z18R1u54cqim"},"source":["trainer.train(style_image=style_image, \n","              test_image=test_image, \n","              checkpoint_model_dir=checkpoints_dir, \n","              epochs=epochs, \n","              content_weight=content_weight, \n","              style_weight=style_weight,\n","              content_scale=content_scale,\n","              style_size=style_size,\n","              log_interval=log_interval, \n","              checkpoint_interval=checkpoint_interval)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jdYeHD4Kg_MB"},"source":["## Export the model to ONNX\n","We can finally export the model to ONNX format. PyTorch exports models by feeding a sample input into the model and tracing what operators are used to compute the outputs.\n","\n","We'll use a `(1, 3, 960, 540)` Tensor with random values as our sample input. This is equivalent to feeding a `960x540` RGB image to the model. The resolution doesn't matter as we can feed images with arbitrary resolutions once the model is exported.\n","\n","The ONNX file will be saved to the project folder in Google Drive.\n","\n","**Note:** You will get a warning after running the code cell below recommending that you use ONNX opset 11 or above. Unity has prioritized support for opset 9 for Barracuda and higher opsets are not fully supported."]},{"cell_type":"code","metadata":{"id":"hUK6RDfSwXxE"},"source":["checkpoint_path = f\"{checkpoints_dir}/final.pth\"\n","style_model = load_checkpoint(checkpoint_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZGOvUXiGH08w","executionInfo":{"status":"ok","timestamp":1614748442917,"user_tz":480,"elapsed":4814426,"user":{"displayName":"cj mills","photoUrl":"","userId":"15496493699191594524"}},"outputId":"1b752de8-6749-4c53-ad5c-6d0a8c2ac5ed"},"source":["x = torch.randn(1, 3, 960, 540).cpu()\n","\n","torch.onnx.export(style_model.cpu(),     #  Model being run\n","                  x,                           # Sample input\n","                  f\"{project_dir}/final.onnx\", # Path to save ONNX file\n","                  export_params=True,          # Store trained weights\n","                  opset_version=9,             # Which ONNX version to use\n","                  do_constant_folding=True     # Replace operations that have all constant inputs with pre-computed nodes\n","                 )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/onnx/symbolic_helper.py:267: UserWarning: You are trying to export the model with onnx:Upsample for ONNX opset version 9. This operator might cause results to not match the expected results by PyTorch.\n","ONNX's Upsample/Resize operator did not match Pytorch's Interpolation until opset 11. Attributes to determine how to transform the input were added in onnx:Resize in opset 11 to support Pytorch's behavior (like coordinate_transformation_mode and nearest_mode).\n","We recommend using opset 11 and above for models using this operator. \n","  \"\" + str(_export_onnx_opset_version) + \". \"\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"xdJO0yRsH0oq"},"source":[""],"execution_count":null,"outputs":[]}]}